word_rnn

0. 网络上疯传的模仿郭敬明写小说、模拟猫哥写歌词<br />
1. 第119行：读入所有.txt文本内容到raw_text后，全部.lower()小写化，然后使用nltk载入英文的语料库english.pickle对raw_text进行tokenize分词得到sents即句子、再对其.word_tokenize()得到单词放入corpus，于是corpus就是list of word即单词构成的数组<br />
2. 第120行：之后，调用Word2Vec函数对corpus里的所有单词生成w2v坐标向量，size=128代表要生成的向量是128维的、window=5代表一次考虑前后5个单词，使用w2v_model就可以查看office这个单词对应的128维向量了，这就相当于把所有的字符都“量化”了，机器就可以处理了（机器只能处理向量、数字这种，你给它直接一个“你好”、hello这样的字符它毕竟都不认得）。<br />
   这一步也是本节猜下一个单词跟之前节猜下一个字母的最大不同，这里得用Word2Vec的方式得到一个单词中所有字母的w2v坐标均值。而之前节我们直接自己构建语料库和字符编码表（0到61）即可。<br />
3. 第122行：可以看出，此刻一共有2115170个单词。。。这中间第122到124行不是特别明确是做什么的，反正就是把2115170长度的raw_input转成了2058753长度的text_stream，视频也未仔细讲。总之，我们得到训练集为text_stream。这里因为从训练集得到的输入raw_input是按照句子都是单词有序的，所以你这样添加进text_stream的也都是有序的，于是就能训练前10个单词预测最后一个单词了。<br />
4. 第125行：seq_length=10代表我们考虑前面10个单词构成的序列作为x（features），y是第11个单词也就是我们要预测的对象（target）。于是这里我们构建好训练集的features即x和训练集的target因变量即y<br />
   这里.html上写错了，x是前置单词们，不是字母们，y也是单词而不是字母<br />
5. 第126行：于是这里我们可以查看，x是前10个单词的w2v坐标集合（每个元素是一个单词，每个单词都是128维的，对应一个128维w2v坐标），而y是第11个单词（我们要预测的单词）的w2v坐标（也是128维）。<br />
6. 第129行：这之前的步骤跟简单版的<用RNN之LSTM做文本预测（猜下一个字母）char_rnn>一节是一样的，这里不是特别明白为什么LSTM是256维的。具体解释请参考前节。后面就是.fit跑模型看结果。<br />